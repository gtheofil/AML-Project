{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import welch\n",
    "import os\n",
    "\n",
    "def process_emg_folder(input_folder, output_folder, shuffle_order_file):\n",
    "    \"\"\"\n",
    "    遍历 input_folder 下的所有 CSV 文件，处理 EMG 和 IMU 数据，并保存特征矩阵和标签到 output_folder。\n",
    "    \"\"\"\n",
    "    # 设定采样率\n",
    "    fs = 1000  # 采样率 Hz\n",
    "    cycle_duration = 10  # 每个周期 10 秒\n",
    "    skip_seconds = 5  # 跳过前 4 秒\n",
    "    use_seconds = 5  # 需要保留的秒数\n",
    "\n",
    "    cycle_samples = fs * cycle_duration  # 10 秒数据点数 = 10000\n",
    "    skip_samples = fs * skip_seconds  # 跳过 4 秒 = 4000\n",
    "    use_samples = fs * use_seconds  # 取后 6 秒 = 6000\n",
    "\n",
    "    # 滑动窗口参数\n",
    "    window_size = 400  # 200ms = 200 采样点\n",
    "    step_size = 200  # 100ms = 100 采样点\n",
    "    num_windows = (use_samples - window_size) // step_size + 1  # 计算窗口数\n",
    "\n",
    "    # 创建存储文件夹\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # **读取 shuffle_order.xlsx**\n",
    "    shuffle_df = pd.read_excel(shuffle_order_file, engine=\"openpyxl\")\n",
    "    if shuffle_df.shape[0] < 15:\n",
    "        raise ValueError(\"标签文件数据不足 15 组！请检查 `shuffle_order.xlsx`。\")\n",
    "\n",
    "    # 存储所有数据和标签\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    # **遍历 CSV 文件**\n",
    "    csv_files = sorted([f for f in os.listdir(input_folder) if f.endswith(\".csv\")])\n",
    "\n",
    "    for file_idx, file_name in enumerate(csv_files):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        print(f\"Processing {file_name} ({file_idx+1}/{len(csv_files)})\")\n",
    "\n",
    "        # **读取数据**\n",
    "        raw_data = pd.read_csv(file_path)\n",
    "\n",
    "        # **检查通道数（忽略时间列）**\n",
    "        num_channels = raw_data.shape[1] - 1\n",
    "        print(f\"Detected {num_channels} channels (excluding time column).\")\n",
    "\n",
    "        # **归一化（Z-score 标准化）**\n",
    "        data = raw_data.iloc[:, 1:]  # 去掉时间列\n",
    "        mean_vals = data.mean(axis=0)\n",
    "        std_vals = data.std(axis=0)\n",
    "        normalized_data = (data - mean_vals) / (std_vals + 1e-10)  # 避免除零错误\n",
    "\n",
    "        # **更新原始数据**\n",
    "        raw_data.iloc[:, 1:] = normalized_data.astype(np.float64)\n",
    "\n",
    "        segments = []  # 存储所有分割后的数据\n",
    "        labels = shuffle_df.iloc[file_idx].values.tolist()  # 获取该文件对应的 26 组标签\n",
    "        num_cycles = 26  # 每个文件固定 26 段（每段 10s）\n",
    "\n",
    "        for i in range(num_cycles):\n",
    "            start_idx = i * cycle_samples + skip_samples  # 跳过前 4 秒\n",
    "            end_idx = start_idx + use_samples  # 取后 6 秒\n",
    "\n",
    "            if end_idx > len(raw_data):  # 处理不足 6000 采样点的情况\n",
    "                segment = raw_data.iloc[start_idx:].values  # 取剩余数据\n",
    "                pad_size = use_samples - len(segment)  # 计算填充数\n",
    "                segment = np.pad(segment, ((0, pad_size), (0, 0)), mode='constant', constant_values=0)  # 填充 0\n",
    "            else:\n",
    "                segment = raw_data.iloc[start_idx:end_idx].values  # 正常提取数据\n",
    "\n",
    "            # **滑动窗口**\n",
    "            windows = [\n",
    "                segment[j:j + window_size, 1:]  # 取 200 采样点，忽略时间列\n",
    "                for j in range(0, use_samples - window_size + 1, step_size)  # 滑动步长 100\n",
    "            ]\n",
    "            segments.append(np.array(windows))\n",
    "\n",
    "        # **转换为 NumPy 数组**\n",
    "        segments_array = np.array(segments)  # 形状 (26, num_windows, 200, num_channels)\n",
    "\n",
    "        # **计算特征**\n",
    "        features_batches = []\n",
    "        for batch_idx in range(segments_array.shape[0]):  # 26 个 batch\n",
    "            batch_features = []  # 存储当前 batch 的所有窗口特征\n",
    "            for window_idx in range(segments_array.shape[1]):  # 计算每个窗口\n",
    "                window = segments_array[batch_idx, window_idx]  # (200, num_channels)\n",
    "                features = extract_features(window)  # 计算 (15, 10)\n",
    "                batch_features.append(features)  # 存入 batch\n",
    "\n",
    "            features_batches.append(np.array(batch_features))\n",
    "\n",
    "        features_array = np.array(features_batches)  # (26, num_windows, 15, 10)\n",
    "\n",
    "        # **存储数据**\n",
    "        all_features.append(features_array)\n",
    "        all_labels.extend(labels)  # 每个文件 26个 cycle，取 shuffle_order.xlsx 里的前 26 个标签\n",
    "\n",
    "    # **最终转换为 NumPy 数组**\n",
    "    all_features = np.vstack(all_features)  # 合并所有 batch，形状 (总 batch, num_windows, 15, 10)\n",
    "    all_labels = np.array(all_labels)  # (总 batch,)\n",
    "\n",
    "    # **保存**\n",
    "    np.save(os.path.join(output_folder, \"feature_matrix.npy\"), all_features)\n",
    "    np.save(os.path.join(output_folder, \"labels.npy\"), all_labels)\n",
    "\n",
    "    print(f\"Feature extraction complete! Shape: {all_features.shape}\")\n",
    "    print(f\"Labels saved: {all_labels.shape}\")\n",
    "    print(f\"Feature matrix saved at: {output_folder}/feature_matrix.npy\")\n",
    "    print(f\"Labels saved at: {output_folder}/labels.npy\")\n",
    "\n",
    "\n",
    "def extract_features(segment):\n",
    "    \"\"\"\n",
    "    计算 EMG 和 IMU 的 15 个特征，并拼接成 (15, 10)\n",
    "    \"\"\"\n",
    "    # **分离 EMG (前四列) 和 IMU (后六列)**\n",
    "    emg_signals = segment[:, :4]  # 4 个 EMG 通道 (200, 4)\n",
    "    imu_signals = segment[:, 4:]  # 6 个 IMU 通道 (200, 6)\n",
    "\n",
    "    def compute_emg_features(signal):\n",
    "        return np.array([\n",
    "            np.var(signal), np.mean(np.abs(signal)), np.sqrt(np.mean(signal**2)),\n",
    "            np.std(signal), np.mean(np.abs(np.diff(signal))), np.max(signal),\n",
    "            np.min(signal), np.sum(np.diff(signal) > 0), np.sum(np.diff(np.sign(signal)) != 0),\n",
    "            stats.kurtosis(signal), stats.skew(signal), np.sum(np.abs(np.diff(signal))),\n",
    "            np.sum(signal ** 2), np.log10(np.mean(signal**2) + 1e-10),\n",
    "            np.mean(welch(signal, fs=1000, nperseg=200)[1])\n",
    "        ])\n",
    "\n",
    "    def compute_imu_features(signal):\n",
    "        return np.array([\n",
    "            np.var(signal), np.mean(signal), np.sqrt(np.mean(signal**2)),\n",
    "            np.std(signal), np.max(signal), np.min(signal),\n",
    "            stats.kurtosis(signal), stats.skew(signal),\n",
    "            np.mean(np.abs(np.diff(signal))), np.sum(np.abs(np.diff(signal))),\n",
    "            np.sum(signal ** 2), np.log10(np.mean(signal**2) + 1e-10),\n",
    "            np.mean(welch(signal, fs=1000, nperseg=200)[1]), np.median(signal), np.ptp(signal)\n",
    "        ])\n",
    "\n",
    "    emg_features = np.array([compute_emg_features(emg_signals[:, i]) for i in range(4)]).T  # (15, 4)\n",
    "    imu_features = np.array([compute_imu_features(imu_signals[:, i]) for i in range(6)]).T  # (15, 6)\n",
    "\n",
    "    return np.concatenate((emg_features, imu_features), axis=1)  # (15, 10)\n",
    "\n",
    "# **运行函数**\n",
    "root = r\"data\\G\"\n",
    "input_folder = root\n",
    "output_folder = os.path.join(root, \"windowed_data\")\n",
    "shuffle_order_file = os.path.join(root,\"shuffle_order.xlsx\")\n",
    "process_emg_folder(input_folder, output_folder, shuffle_order_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import welch\n",
    "import os\n",
    "\n",
    "def process_emg_folder(input_folder, output_folder, shuffle_order_file):\n",
    "    \"\"\"\n",
    "    遍历 input_folder 下的所有 CSV 文件，处理 EMG 和 IMU 数据，并保存特征矩阵和标签到 output_folder。\n",
    "    \"\"\"\n",
    "    # 设定采样率\n",
    "    original_fs = 1000  # 原采样率 Hz\n",
    "    target_fs = 200  # 目标采样率 Hz\n",
    "    downsample_factor = original_fs // target_fs  # 降采样因子\n",
    "    cycle_duration = 10  # 每个周期 10 秒\n",
    "    skip_seconds = 5  # 跳过前 4 秒\n",
    "    use_seconds = 5  # 需要保留的秒数\n",
    "\n",
    "    cycle_samples = (original_fs * cycle_duration) // downsample_factor  # 2000\n",
    "    skip_samples = (original_fs * skip_seconds) // downsample_factor  # 1000\n",
    "    use_samples = (original_fs * use_seconds) // downsample_factor  # 1000\n",
    "\n",
    "    # 滑动窗口参数\n",
    "    window_size = 100  # 200ms = 40 采样点 (原 200, 降采样后 40)\n",
    "    step_size = 50  # 100ms = 20 采样点 (原 100, 降采样后 20)\n",
    "    num_windows = (use_samples - window_size) // step_size + 1  # 计算窗口数\n",
    "\n",
    "    # 创建存储文件夹\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 读取 shuffle_order.xlsx\n",
    "    shuffle_df = pd.read_excel(shuffle_order_file, engine=\"openpyxl\")\n",
    "    if shuffle_df.shape[0] < 15:\n",
    "        raise ValueError(\"标签文件数据不足 15 组！请检查 `shuffle_order.xlsx`。\")\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    csv_files = sorted([f for f in os.listdir(input_folder) if f.endswith(\".csv\")])\n",
    "\n",
    "    for file_idx, file_name in enumerate(csv_files):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        print(f\"Processing {file_name} ({file_idx+1}/{len(csv_files)})\")\n",
    "\n",
    "        raw_data = pd.read_csv(file_path)\n",
    "        num_channels = raw_data.shape[1] - 1\n",
    "        print(f\"Detected {num_channels} channels (excluding time column).\")\n",
    "\n",
    "        # 降采样\n",
    "        raw_data = raw_data.iloc[::downsample_factor, :].reset_index(drop=True)\n",
    "        \n",
    "        # 归一化\n",
    "        data = raw_data.iloc[:, 1:]\n",
    "        mean_vals = data.mean(axis=0)\n",
    "        std_vals = data.std(axis=0)\n",
    "        normalized_data = (data - mean_vals) / (std_vals + 1e-10)\n",
    "        raw_data.iloc[:, 1:] = normalized_data.astype(np.float64)\n",
    "\n",
    "        segments = []\n",
    "        labels = shuffle_df.iloc[file_idx].values.tolist()\n",
    "        num_cycles = 26\n",
    "\n",
    "        for i in range(num_cycles):\n",
    "            start_idx = i * cycle_samples + skip_samples\n",
    "            end_idx = start_idx + use_samples\n",
    "\n",
    "            if end_idx > len(raw_data):\n",
    "                segment = raw_data.iloc[start_idx:].values\n",
    "                pad_size = use_samples - len(segment)\n",
    "                segment = np.pad(segment, ((0, pad_size), (0, 0)), mode='constant', constant_values=0)\n",
    "            else:\n",
    "                segment = raw_data.iloc[start_idx:end_idx].values\n",
    "\n",
    "            windows = [\n",
    "                segment[j:j + window_size, 1:]\n",
    "                for j in range(0, use_samples - window_size + 1, step_size)\n",
    "            ]\n",
    "            segments.append(np.array(windows))\n",
    "\n",
    "        segments_array = np.array(segments)\n",
    "        \n",
    "        features_batches = []\n",
    "        for batch_idx in range(segments_array.shape[0]):\n",
    "            batch_features = []\n",
    "            for window_idx in range(segments_array.shape[1]):\n",
    "                window = segments_array[batch_idx, window_idx]\n",
    "                features = extract_features(window)\n",
    "                batch_features.append(features)\n",
    "\n",
    "            features_batches.append(np.array(batch_features))\n",
    "\n",
    "        features_array = np.array(features_batches)\n",
    "\n",
    "        all_features.append(features_array)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_features = np.vstack(all_features)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    np.save(os.path.join(output_folder, \"feature_matrix.npy\"), all_features)\n",
    "    np.save(os.path.join(output_folder, \"labels.npy\"), all_labels)\n",
    "\n",
    "    print(f\"Feature extraction complete! Shape: {all_features.shape}\")\n",
    "    print(f\"Labels saved: {all_labels.shape}\")\n",
    "    print(f\"Feature matrix saved at: {output_folder}/feature_matrix.npy\")\n",
    "    print(f\"Labels saved at: {output_folder}/labels.npy\")\n",
    "\n",
    "def extract_features(segment):\n",
    "    emg_signals = segment[:, :4]\n",
    "    imu_signals = segment[:, 4:]\n",
    "\n",
    "    def compute_emg_features(signal):\n",
    "        return np.array([\n",
    "            np.var(signal), np.mean(np.abs(signal)), np.sqrt(np.mean(signal**2)),\n",
    "            np.std(signal), np.mean(np.abs(np.diff(signal))), np.max(signal),\n",
    "            np.min(signal), np.sum(np.diff(signal) > 0), np.sum(np.diff(np.sign(signal)) != 0),\n",
    "            stats.kurtosis(signal), stats.skew(signal), np.sum(np.abs(np.diff(signal))),\n",
    "            np.sum(signal ** 2), np.log10(np.mean(signal**2) + 1e-10),\n",
    "            np.mean(welch(signal, fs=200, nperseg=40)[1])\n",
    "        ])\n",
    "\n",
    "    def compute_imu_features(signal):\n",
    "        return np.array([\n",
    "            np.var(signal), np.mean(signal), np.sqrt(np.mean(signal**2)),\n",
    "            np.std(signal), np.max(signal), np.min(signal),\n",
    "            stats.kurtosis(signal), stats.skew(signal),\n",
    "            np.mean(np.abs(np.diff(signal))), np.sum(np.abs(np.diff(signal))),\n",
    "            np.sum(signal ** 2), np.log10(np.mean(signal**2) + 1e-10),\n",
    "            np.mean(welch(signal, fs=200, nperseg=40)[1]), np.median(signal), np.ptp(signal)\n",
    "        ])\n",
    "\n",
    "    emg_features = np.array([compute_emg_features(emg_signals[:, i]) for i in range(4)]).T\n",
    "    imu_features = np.array([compute_imu_features(imu_signals[:, i]) for i in range(6)]).T\n",
    "\n",
    "    return np.concatenate((emg_features, imu_features), axis=1)\n",
    "\n",
    "root = r\"data\\G\"\n",
    "input_folder = root\n",
    "output_folder = os.path.join(root, \"windowed_data\")\n",
    "shuffle_order_file = os.path.join(root, \"shuffle_order.xlsx\")\n",
    "process_emg_folder(input_folder, output_folder, shuffle_order_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded features from E:\\MSC\\Spring\\AML\\GestureLink\\data\\G\\windowed_data\\feature_matrix.npy, shape: (390, 19, 15, 10)\n",
      "Loaded labels from E:\\MSC\\Spring\\AML\\GestureLink\\data\\G\\windowed_data\\labels.npy, shape: (390,)\n",
      "Training set: X_train: (312, 19, 15, 10), y_train: (312,)\n",
      "Testing set: X_test: (78, 19, 15, 10), y_test: (78,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(data_folder, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    加载 `feature_matrix.npy` 和 `labels.npy` 数据，并划分训练集和测试集。\n",
    "\n",
    "    参数：\n",
    "    - data_folder: 存放数据的文件夹路径\n",
    "    - test_size: 测试集比例 (默认 20%)\n",
    "    - random_state: 随机种子，保证可复现性\n",
    "\n",
    "    返回：\n",
    "    - X_train: 训练集特征，形状 (train_batches, num_windows, 15, num_channels)\n",
    "    - X_test: 测试集特征，形状 (test_batches, num_windows, 15, num_channels)\n",
    "    - y_train: 训练集标签，形状 (train_batches,)\n",
    "    - y_test: 测试集标签，形状 (test_batches,)\n",
    "    \"\"\"\n",
    "    # **加载数据**\n",
    "    feature_path = os.path.join(data_folder, \"feature_matrix.npy\")\n",
    "    label_path = os.path.join(data_folder, \"labels.npy\")\n",
    "\n",
    "    if not os.path.exists(feature_path) or not os.path.exists(label_path):\n",
    "        raise FileNotFoundError(\"特征文件或标签文件未找到，请检查路径！\")\n",
    "\n",
    "    X = np.load(feature_path)  # 形状 (num_batches, num_windows, 15, num_channels)\n",
    "    y = np.load(label_path)  # 形状 (num_batches,)\n",
    "\n",
    "    # **数据基本信息**\n",
    "    print(f\"Loaded features from {feature_path}, shape: {X.shape}\")\n",
    "    print(f\"Loaded labels from {label_path}, shape: {y.shape}\")\n",
    "\n",
    "    # **划分训练集和测试集**\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "\n",
    "    # **打印数据划分信息**\n",
    "    print(f\"Training set: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"Testing set: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# **使用示例**\n",
    "data_folder = r\"E:\\MSC\\Spring\\AML\\GestureLink\\data\\G\\windowed_data\"\n",
    "X_train, X_test, y_train, y_test = load_data(data_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理分开的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import welch\n",
    "\n",
    "def process_emg_folder(input_folder, output_folder, shuffle_order_file):\n",
    "    \"\"\"\n",
    "    处理 EMG 数据文件，自动检测手势起点，并确保每个手势数据长度为 5000 采样点。\n",
    "    \"\"\"\n",
    "    fs = 1000  # 采样率 1000Hz，每秒 1000 个数据点\n",
    "    target_samples = 5000  # 每个手势的固定采样点数\n",
    "    gap_threshold = 500  # 采样点间隔超过 500 认为是新手势\n",
    "\n",
    "    # 创建输出文件夹\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 读取 shuffle_order.xlsx\n",
    "    shuffle_df = pd.read_excel(shuffle_order_file, engine=\"openpyxl\")\n",
    "    if shuffle_df.shape[0] < 26:\n",
    "        raise ValueError(\"标签文件数据不足 26 组！请检查 `shuffle_order.xlsx`。\")\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    csv_files = sorted([f for f in os.listdir(input_folder) if f.endswith(\".csv\")])\n",
    "\n",
    "    for file_idx, file_name in enumerate(csv_files):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        print(f\"Processing {file_name} ({file_idx + 1}/{len(csv_files)})\")\n",
    "\n",
    "        # 读取数据\n",
    "        raw_data = pd.read_csv(file_path)\n",
    "        num_channels = raw_data.shape[1] - 1  # 去掉时间列\n",
    "        print(f\"Detected {num_channels} channels (excluding time column).\")\n",
    "\n",
    "        # 归一化数据\n",
    "        data = raw_data.iloc[:, 1:]\n",
    "        normalized_data = (data - data.mean(axis=0)) / (data.std(axis=0) + 1e-10)\n",
    "        raw_data.iloc[:, 1:] = normalized_data.astype(np.float64)\n",
    "\n",
    "        # 检测手势起点\n",
    "        valid_segments = []\n",
    "        start_idx = 0\n",
    "        first_handled = False  # 确保第一组数据从0开始\n",
    "\n",
    "        for i in range(1, len(raw_data)):\n",
    "            if (raw_data.index[i] - raw_data.index[i - 1]) > gap_threshold:\n",
    "                if not first_handled:  # 处理第一组手势\n",
    "                    segment = raw_data.iloc[:target_samples, 1:].values\n",
    "                    if len(segment) < target_samples:\n",
    "                        pad_size = target_samples - len(segment)\n",
    "                        segment = np.pad(segment, ((0, pad_size), (0, 0)), mode='constant', constant_values=0)\n",
    "                    valid_segments.append(segment)\n",
    "                    first_handled = True\n",
    "\n",
    "                if i - start_idx >= target_samples:\n",
    "                    segment = raw_data.iloc[start_idx:start_idx + target_samples, 1:].values\n",
    "                    valid_segments.append(segment)\n",
    "                start_idx = i\n",
    "\n",
    "        # 处理最后一段\n",
    "        if len(raw_data) - start_idx >= target_samples:\n",
    "            segment = raw_data.iloc[start_idx:start_idx + target_samples, 1:].values\n",
    "            valid_segments.append(segment)\n",
    "\n",
    "        # 计算特征\n",
    "        processed_segments = np.array([extract_features(seg) for seg in valid_segments])  # (num_gestures, 15, num_channels)\n",
    "\n",
    "        if len(processed_segments) != 26:\n",
    "            print(f\"Warning: {file_name} 提取到 {len(processed_segments)} 个手势，不是 26 个。\")\n",
    "\n",
    "        # 存储数据\n",
    "        all_features.append(processed_segments)\n",
    "        all_labels.extend(shuffle_df.iloc[file_idx, :26].values.tolist())\n",
    "\n",
    "    # 最终转换为 NumPy 数组\n",
    "    all_features = np.vstack(all_features)  # 形状 (总手势数, 15, num_channels)\n",
    "    all_labels = np.array(all_labels)  # (总手势数,)\n",
    "\n",
    "    # 保存数据\n",
    "    np.save(os.path.join(output_folder, \"feature_matrix.npy\"), all_features)\n",
    "    np.save(os.path.join(output_folder, \"labels.npy\"), all_labels)\n",
    "\n",
    "    print(f\"Feature extraction complete! Shape: {all_features.shape}\")\n",
    "    print(f\"Labels saved: {all_labels.shape}\")\n",
    "    print(f\"Feature matrix saved at: {output_folder}/feature_matrix.npy\")\n",
    "    print(f\"Labels saved at: {output_folder}/labels.npy\")\n",
    "\n",
    "def extract_features(segment):\n",
    "    \"\"\"\n",
    "    计算 EMG 和 IMU 的特征。\n",
    "    \"\"\"\n",
    "    emg_signals = segment[:, :4]  # 4 个 EMG 通道\n",
    "    imu_signals = segment[:, 4:]  # 6 个 IMU 通道\n",
    "\n",
    "    def compute_emg_features(signal):\n",
    "        return np.array([\n",
    "            np.var(signal), np.mean(np.abs(signal)), np.sqrt(np.mean(signal**2)),\n",
    "            np.std(signal), np.mean(np.abs(np.diff(signal))), np.max(signal),\n",
    "            np.min(signal), np.sum(np.diff(signal) > 0), np.sum(np.diff(np.sign(signal)) != 0),\n",
    "            stats.kurtosis(signal), stats.skew(signal), np.sum(np.abs(np.diff(signal))),\n",
    "            np.sum(signal ** 2), np.log10(np.mean(signal**2) + 1e-10),\n",
    "            np.mean(welch(signal, fs=1000, nperseg=200)[1])\n",
    "        ])\n",
    "\n",
    "    def compute_imu_features(signal):\n",
    "        return np.array([\n",
    "            np.var(signal), np.mean(signal), np.sqrt(np.mean(signal**2)),\n",
    "            np.std(signal), np.max(signal), np.min(signal),\n",
    "            stats.kurtosis(signal), stats.skew(signal),\n",
    "            np.mean(np.abs(np.diff(signal))), np.sum(np.abs(np.diff(signal))),\n",
    "            np.sum(signal ** 2), np.log10(np.mean(signal**2) + 1e-10),\n",
    "            np.mean(welch(signal, fs=1000, nperseg=200)[1]), np.median(signal), np.ptp(signal)\n",
    "        ])\n",
    "\n",
    "    emg_features = np.array([compute_emg_features(emg_signals[:, i]) for i in range(4)]).T  # (15, 4)\n",
    "    imu_features = np.array([compute_imu_features(imu_signals[:, i]) for i in range(6)]).T  # (15, 6)\n",
    "\n",
    "    return np.concatenate((emg_features, imu_features), axis=1)  # (15, 10)\n",
    "\n",
    "# 运行代码\n",
    "root = r\"data\\ZHF\"\n",
    "input_folder = root\n",
    "output_folder = os.path.join(root, \"processed_data\")\n",
    "shuffle_order_file = os.path.join(root, \"shuffle_order.xlsx\")\n",
    "process_emg_folder(input_folder, output_folder, shuffle_order_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.20 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2403de98e458f1581c3ddc5cac98c8c9b462b82dd05a074a785d43eb5bf629fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
